from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Create the DataFrame
data = [
    (1, 1, 0), (1, 2, 0), (1, 3, 0), (1, 4, 0), (1, 5, 0),
    (1, 6, 0), (1, 7, 0), (1, 8, 0), (1, 9, 0), (1, 10, 1),
    (2, 1, 0), (2, 2, 0), (2, 3, 0), (2, 4, 0), (2, 5, 0),
    (2, 6, 0), (2, 7, 0), (2, 8, 0), (2, 9, 1), (2, 10, 1),
    (3, 1, 0), (3, 2, 0), (3, 3, 1), (3, 4, 1), (3, 5, 1),
    (3, 6, 1), (3, 7, 0), (3, 8, 1), (3, 9, 1), (3, 10, 1),
    (4, 1, 1), (4, 2, 0), (4, 3, 0), (4, 4, 0), (4, 5, 0),
    (4, 6, 0), (4, 7, 1), (4, 8, 1), (4, 9, 1), (4, 10, 1)
]
df = spark.createDataFrame(data, ["Partition", "Order", "Flags"])

# Define the window specification
window_spec = Window.partitionBy("Partition").orderBy("Order")

# Apply the logic using window functions
df = df.withColumn(
    "ShouldKeep_Correct",
    F.when(
        (F.col("Flags") == 1) & (F.lead("Flags").over(window_spec) != 1),
        True
    ).otherwise(False)
)

# Filter the rows based on the logic
df = df.filter(F.col("ShouldKeep") == True)

# Show the output
df.show()
