from pyspark.sql import functions as F
from pyspark.sql.window import Window

def find_pattern_start(df, value_column, flag_column):
    window_spec = Window.orderBy('id')
    
    return (df
        .withColumn('id', F.monotonically_increasing_id())
        # Find where new sequence starts
        .withColumn('seq_start',
            F.when(
                (F.col(value_column) != F.lag(F.col(value_column), 1).over(window_spec)) |
                F.lag(F.col(value_column), 1).over(window_spec).isNull(),
                1
            ).otherwise(0))
        # Mark the sequence start that corresponds to flag=1
        .withColumn('result',
            F.when(
                (F.col('seq_start') == 1) &
                (F.col(value_column) == F.first(
                    F.when(F.col(flag_column) == 1, F.col(value_column))
                    .otherwise(None)
                ).over(Window.orderBy('id').rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing))),
                1
            ).otherwise(0))
        # Keep only the occurrence that matches the flag position
        .withColumn('result',
            F.when(
                F.sum(F.col('result')).over(Window.orderBy('id').rowsBetween(Window.unboundedPreceding, 0)) <= 
                F.sum(F.col(flag_column)).over(Window.orderBy('id').rowsBetween(Window.unboundedPreceding, 0)),
                F.col('result')
            ).otherwise(0))
        .select('result'))

# Example usage
data = [('c','0'), ('c','0'), ('a','0'), ('a','0'), ('b','0'), ('b','0'), 
        ('c','0'), ('c','0'), ('a','0'), ('a','0'), ('a','0'), ('a','1'), 
        ('b','0'), ('b','0')]
df = spark.createDataFrame(data, ['letter', 'flag'])
result_df = find_pattern_start(df, 'letter', 'flag')
